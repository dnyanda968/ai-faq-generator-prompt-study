# AI FAQ Generator Prompt Study

This project explores how prompt engineering affects the quality of FAQ extraction from a policy document.

## 📂 Project Structure

- `sample_input.txt` – Example company policy document.
- `Prompts.pdf` – Three prompt variations.
- `Outputs/` – Outputs generated for each prompt.

## 💡 Overview

I tested three prompt styles:

1. **Simple Q&A** – Basic extraction of questions and answers.
2. **Detailed Q&A** – More specific, thorough responses.
3. **Markdown Table** – Structured output in table format.

## 🛠️ Tools Used

- ChatGPT for prompt testing
- GitHub for version control

## ✨ Key Learnings

- Prompt specificity increases the detail in outputs.
- Output formatting instructions affect readability.
- Iterating on prompts improves clarity.

